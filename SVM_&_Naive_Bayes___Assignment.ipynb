{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ramCn4KvRap6"
      },
      "outputs": [],
      "source": [
        "## Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "\"\"\"\n",
        "Answer: A Support Vector Machine (SVM) is a supervised machine learning technique that determines the appropriate \"hyperplane\" for categorising input points and maximising\n",
        "the margin between them. It operates by detecting \"support vectors\"—the data points nearest to the hyperplane—that establish the border, and it employs a \"kernel trick\" to\n",
        "handle non-linear data by transferring it to a higher-dimensional space where it may be linearly separated.\n",
        "SVM works in the following maner:\n",
        "Finding the Optimal Hyperplane: The primary purpose of an SVM is to identify the hyperplane that gives the greatest separation between distinct data classes.\n",
        "Define the Margin: This separation is measured by the \"margin,\" which is the distance between the hyperplane and the nearest data points from each class.\n",
        "Identifying Support Vectors: The data points on the edge of this margin are referred to as support vectors. These are the most essential sites since removing or moving\n",
        "them may cause the hyperplane to alter.\n",
        "Classification: Once the ideal hyperplane has been identified (in either the original or a higher-dimensional space), it is used to categorise new, previously unseen datasets.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Question 2: Explain the difference between Hard Margin and Soft Margin SVM\n",
        "\n",
        "\"\"\"\n",
        "Answer: Hard Margin SVM requires perfectly linearly separable data and insists that no data points fall within the margin or on the incorrect side of the decision boundary,\n",
        "making it sensitive to outliers. In contrast, Soft Margin SVM allows for some misclassifications and margin violations via slack variables, resulting in a more flexible\n",
        "and robust model capable of handling imperfect real-world data containing outliers or overlapping classes.\n",
        "Hard Margin SVMs aim for complete separation, which can lead to overfitting on noisy data, whereas Soft Margin SVMs \"embrace the messiness of reality\" by embracing imperfect\n",
        "boundaries to increase generalisation on complicated, real-world data.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "enWcTYiZSpKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "\n",
        "\"\"\"\n",
        "Answer: The Kernel Trick is an SVM approach that implicitly transfers non-linearly separable data into a higher-dimensional space, making it linearly separable and allowing\n",
        "a linear classifier to locate a boundary. It avoids explicitly computing high-dimensional coordinates by computing the dot product of vectors in the higher space with a\n",
        "kernel function. A common example is the Radial Basis Function (RBF) Kernel, which maps data to an infinite-dimensional space to handle complicated patterns, resulting\n",
        "in a nonlinear decision boundary in the original space and a linear plane in the higher dimension.\n",
        "In many cases, data cannot be divided into distinct classes using a simple straight line (linear boundary) in its original low-dimensional space.\n",
        "Solution: The kernel trick suggests transforming this non-linearly separable data into a higher-dimensional feature space that can be separated by a linear hyperplane.\n",
        "Instead of executing the computationally intensive transformation (mapping each data point into the new space), a kernel function is utilised. This function takes two\n",
        "original data points and calculates their dot product in a higher-dimensional space.The SVM method then operates on these dot products, thereby locating the linear separator\n",
        "in the higher-dimensional space, resulting in a nonlinear decision boundary in the original data space.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "n-pFEiNKTY7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "\"\"\"\n",
        "Answer: A Naïve Bayes classifier is a simple probabilistic classification technique that uses Bayes' Theorem. It assumes that all features used for classification are\n",
        "independent of each other inside the class. The \"naïve\" algorithm's premise of feature independence may be unrealistic in real-world data, yet it nonetheless performs well\n",
        "in tasks such as text classification and spam detection.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nJLFDcrGUveD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.When would you use each one?\n",
        "\n",
        "\"\"\"\n",
        "Answer: There are three types of Naïve Bayes: Gaussian Naïve Bayes for continuous data with a normal distribution, Multinomial Naive Bayes for discrete data with feature\n",
        "counts like word frequencies, and Bernoulli Naïve Bayes for binary features with presence or absence. These models are suitable for various classification tasks.\n",
        "Gaussian Naive Bayes is used for classification problems requiring continuous numerical data, such as estimating property prices based on variables like size or income,\n",
        "or classifying medical data based on symptoms like age or weight.\n",
        "Multinominal Naive Bayes is ideal for text classification tasks like spam detection and document categorisation, where features represent the frequency of words\n",
        "in a document.\n",
        "Bernoulli Naïve Bayes is used for Suitable for text classification, when a word's presence, rather than frequency, indicates the category. For example, deciding\n",
        "if a document is favourable or negative based on the presence or lack of specific terms.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-W7dbZ4QWm6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an SVM Classifier with a linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Calculating and printing the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print the support vectors\n",
        "print(\"\\nSupport Vectors:\")\n",
        "print(svm_model.support_vectors_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTnHsADdYFPW",
        "outputId": "41f65fab-fb5c-4126-a60e-56dc5117ff90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n",
            "\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 7: Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "gnb_model = GaussianNB()\n",
        "gnb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb_model.predict(X_test)\n",
        "\n",
        "# Printing the classification report including precision, recall, and F1-score\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=breast_cancer.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txUAjYHiYwjr",
        "outputId": "53c14728-5df3-46c2-fb3e-82ca80523453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "svm = SVC(kernel='rbf')\n",
        "\n",
        "grid = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Printing the best hyperparameters and accuracy\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(f\"C: {grid.best_params_['C']}\")\n",
        "print(f\"gamma: {grid.best_params_['gamma']}\")\n",
        "print(f\"\\nBest Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSfELVWMZXSG",
        "outputId": "8468d963-8080-4106-8123-7e38ceb4e96f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "C: 100\n",
            "gamma: 0.001\n",
            "\n",
            "Best Model Accuracy: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "\"\"\"\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# Loading the 20 Newsgroups dataset\n",
        "newsgroups = fetch_20newsgroups(\n",
        "    subset='train',\n",
        "    remove=('headers', 'footers', 'quotes')\n",
        ")\n",
        "X_text = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "    X_text, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(X_train_text)\n",
        "X_test = vectorizer.transform(X_test_text)\n",
        "\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "y_prob = nb_model.predict_proba(X_test)\n",
        "\n",
        "# Computing and printing the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob, multi_class='ovr', average='macro')\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbSWYIKEZ9Q0",
        "outputId": "9d897a16-285f-4cbd-a58b-42c334e94a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 10: Imagine you’re working as a data scientist for a company that handles email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics And explain the business impact of your solution.\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "Answer: As a data scientist for an email communications company, my goal would be to create a reliable spam detection system that minimises user inconvenience while\n",
        "efficiently screening malicious emails. Spam classification is a traditional binary text classification problem in which emails are labelled as \"Spam\" (positive class)\n",
        "or \"Not Spam\" (negative class, such as legitimate emails). Given the challenges—diverse language in text, class imbalance (e.g., 90%+ valid emails), and incomplete/missing\n",
        "data—I would use a structured machine learning pipeline. I've outlined the technique in detail below, with a focus on preprocessing, model selection, dealing with imbalance,\n",
        "evaluation, and business impact.\n",
        "Preprocessing Data\n",
        "Text data requires preprocessing before it can be used in machine learning models. Emails frequently include noisy, unstructured language, missing fields, or incomplete entries.\n",
        "Handling Missing or incomplete data:\n",
        "Scan the dataset for any missing values in important fields like as subject, body, sender, or metadata. Incomplete emails may have empty bodies or garbled text.\n",
        "Strategies:\n",
        "Removal: Remove rows that are completely missing the body/subject. For partial missing data, assign simple answers such as \"Unknown\" to subjects or use mean imputation for\n",
        "numerical metadata.\n",
        "Filling gaps: To avoid model crashes, substitute missing bodies in text fields with placeholders. If metadata, like as timestamps, is lacking, discard the rows that are not\n",
        "critical.\n",
        "Outlier Handling: Remove extremely short/long emails, as they may be artefacts.\n",
        "Models cannot process raw text, therefore vectorisation converts cleaned text into numerical features.\n",
        "The preferred method is the TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer from sklearn.feature_extraction.text.TfidfVectorizer because it handles\n",
        "diverse vocabulary by downweighting common phrases (e.g., \"free\" may be prevalent in spam but not discriminatory across all emails) and emphasising rare, instructive\n",
        "terms (e.g., \"Viagra\"). Set options like as max_features=5000-10000 for efficiency and ngram_range=(1,2) to capture phrases (for example, \"free money\").\n",
        "After preprocessing, split into train/validation/test sets (e.g., 70/15/15) using stratified sampling to maintain class distribution.\n",
        "Given the huge dimensionality of the text features (thousands of TF-IDF terms), I would first investigate simple, interpretable models for spam detection. There are two options: Support Vector Machine (SVM) and Naïve Bayes (NB).\n",
        "Comparison:\n",
        "Naïve Bayes assumes feature independence, a simplification that works well with text due to the \"curse of dimensionality.\" It's probabilistic, trains quickly (in O(n) time),\n",
        "and handles sparse data well. However, it may underperform if the features are highly connected.\n",
        "SVM: Locates a hyperplane that maximises the margin between classes in a multidimensional space. It is resilient to irrelevant features and does not presuppose independence,\n",
        "making it ideal for text categorisation. Linear kernels are favoured for speed on large vocabulary sets; RBF kernels could be used if non-linear bounds are required, although\n",
        "they are computationally expensive.\n",
        "I recommend starting with Naïve Bayes as the primary model. because of the following reasons:\n",
        "Efficiency: Spam datasets can be huge (millions of emails), and NB trains quickly without requiring hyperparameter tuning like SVM's C (regularisation).\n",
        "Text performance: NB excels at spam filtering (for example, it is the foundation of many email clients, including early versions of Gmail) because email text frequently\n",
        "approximates the independence assumption—words are conditionally independent given the class.\n",
        "Interpretability: Probabilities are simple to explain (for example, \"This email is 95% spam due to words like 'lottery'\").\n",
        "SVM as a strong alternative or benchmark: If NB's accuracy is insufficient (for example, because to correlated features), move to SVM, which often produces 1-2% higher\n",
        "F1-scores on benchmarks like as the Enron-Spam dataset. To tweak the C parameter of the SVM, I would utilise GridSearchCV.\n",
        "Class imbalance (for example, 95% Not Spam) might bias models towards the dominant class, resulting in high accuracy but poor spam detection (many false negatives).\n",
        "Strategies:\n",
        "Resampling Techniques:\n",
        "Oversampling the minority class: Use imbalanced-learn's SMOTE (Synthetic Minority Over-sampling Technique) to create synthetic spam samples by interpolating existing ones.\n",
        "To prevent overfitting, avoid using pure random oversampling.\n",
        "Undersampling the majority class: Random downsampling Not spam emails, but exercise caution to avoid losing valuable data—for example, combine with oversampling for balance.\n",
        "Evaluating Performance with Appropriate Metrics\n",
        "Accuracy is deceiving due to imbalance. I'd utilise a multi-metric evaluation on the test set.\n",
        "Key metrics:\n",
        "Precision is the proportion of predicted Spam that is actually Spam (TP/(TP + FP)). Users despise valid emails being marked as spam, therefore minimising false positives\n",
        "is critical.\n",
        "Recall (Sensitivity): The proportion of genuine Spam captured (TP/(TP + FN)). Prioritise this to guarantee a high spam detection rate, as missing spam can lead to phishing\n",
        "attacks.\n",
        "F1-Score is the harmonic mean of Precision and Recall (2 * (Precision * Recall) / (Precision + Recall). A balanced metric for unbalanced data; aim for >0.90 macro-averaged.\n",
        "The ROC-AUC score measures the trade-off between true positive rate and false positive rate. Ideal for binary classification with a target >0.95, as it manages imbalance well.\n",
        "Implementing this spam classifier would significantly improve the company's operations and consumer satisfaction:\n",
        "Improved user experience: Users benefit from cleaner inboxes, less frustration, and increased engagement by capturing over 95% of spam (high recall) with few false positives\n",
        "(<1% of valid emails detected). This could increase client retention by 5-10%, according to industry benchmarks.\n",
        "Security and compliance: Early identification of phishing/spam decreases risks such as data breaches and viruses, potentially saving millions in remediation expenses. It also\n",
        "simplifies compliance with legislation such as GDPR/CAN-SPAM by automating filtering.\n",
        "Operational Efficiency: Automating classification scalable to handle large volumes of email (millions per day) without manual inspection, freeing up support workers.\n",
        "Training/inference is inexpensive (NB works on commodity hardware) and can be deployed via APIs (for example, in email servers using Flask/Docker).\n",
        "ROI Quantification: If the organisation processes 1 million emails per day with 5% spam, the model could prevent 50K spam emails from reaching users, resulting in ~$100K+\n",
        "annual savings (e.g., reduced server storage and user complaints). A/B testing after deployment would measure improvements in metrics such as customer satisfaction scores.\n",
        "Scalability and iteration: Begin with batch processing and progress to real-time (e.g., using Kafka streams). Monitor drift (e.g., developing spam strategies) using tools\n",
        "such as Evidently AI, and retrain quarterly.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Download NLTK resources (run once)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Synthetic dataset (replace with pd.read_csv('emails.csv') for real data)\n",
        "data = {\n",
        "    'text': [\n",
        "        'Buy cheap viagra now! Click here for discount.',  # Spam\n",
        "        'Meeting tomorrow at 10 AM in conference room.',  # Not Spam\n",
        "        'Win free lottery tickets today! Urgent offer.',  # Spam\n",
        "        'Your invoice is attached. Please review.',  # Not Spam\n",
        "        'Hello, how are you? Missing subject.',  # Not Spam (incomplete)\n",
        "        'Free money transfer to your account. Act fast!',  # Spam\n",
        "        '',  # Missing/incomplete\n",
        "        'Project update: Sales increased by 20%.',  # Not Spam\n",
        "        'Dear user, your account is suspended. Login now.',  # Spam\n",
        "        'Lunch invitation from team lead.'  # Not Spam\n",
        "    ],\n",
        "    'label': [1, 0, 1, 0, 0, 1, 0, 0, 1, 0]  # 1=Spam, 0=Not Spam\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Handle missing/incomplete data\n",
        "df = df.dropna(subset=['label'])  # Drop rows with missing labels\n",
        "df['text'] = df['text'].fillna('[No Content]')  # Impute missing text\n",
        "df = df[df['text'].str.len() > 10]  # Remove very short/incomplete (outliers)\n",
        "\n",
        "# Text cleaning function\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation/numbers\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Vectorization with TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))  # Limit for demo; handles diverse vocab\n",
        "X = vectorizer.fit_transform(df['clean_text'])\n",
        "y = df['label']\n",
        "\n",
        "# Split (stratified to preserve imbalance)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "print(f\"Training set shape: {X_train.shape}, Imbalance: {np.bincount(y_train)}\")\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Address imbalance first (see next section)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Train Naïve Bayes (primary model)\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Train SVM for comparison\n",
        "svm_model = LinearSVC(class_weight='balanced', random_state=42)  # Handles imbalance\n",
        "svm_model.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Predictions\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "y_prob_nb = nb_model.predict_proba(X_test)[:, 1]  # Prob for positive class\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "y_prob_svm = svm_model.decision_function(X_test)  # Use decision for ROC\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Evaluate Naïve Bayes\n",
        "print(\"Naïve Bayes Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_nb, target_names=['Not Spam', 'Spam']))\n",
        "\n",
        "roc_auc_nb = roc_auc_score(y_test, y_prob_nb)\n",
        "print(f\"Naïve Bayes ROC-AUC: {roc_auc_nb:.4f}\")\n",
        "\n",
        "# Cross-validation F1 (macro for balance)\n",
        "cv_f1_nb = cross_val_score(nb_model, X_train_res, y_train_res, cv=5, scoring='f1_macro')\n",
        "print(f\"Naïve Bayes CV F1-Macro: {cv_f1_nb.mean():.4f} (+/- {cv_f1_nb.std() * 2:.4f})\")\n",
        "\n",
        "# Evaluate SVM for comparison\n",
        "print(\"\\nSVM Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_svm, target_names=['Not Spam', 'Spam']))\n",
        "\n",
        "roc_auc_svm = roc_auc_score(y_test, y_prob_svm)\n",
        "print(f\"SVM ROC-AUC: {roc_auc_svm:.4f}\")\n",
        "\n",
        "cv_f1_svm = cross_val_score(svm_model, X_train_res, y_train_res, cv=5, scoring='f1_macro')\n",
        "print(f\"SVM CV F1-Macro: {cv_f1_svm.mean():.4f} (+/- {cv_f1_svm.std() * 2:.4f})\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-tivRQd2h1nd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}